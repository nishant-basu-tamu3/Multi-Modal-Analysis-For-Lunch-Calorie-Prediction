{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "44YIv6z-goeQ"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical computations and array operations\n",
    "import torch  # PyTorch library for deep learning\n",
    "import torch.nn as nn  # PyTorch module for building neural networks\n",
    "import torch.optim as optim  # PyTorch module for optimization algorithms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split  # Tools for creating datasets and data loaders\n",
    "import ast  # For processing Abstract Syntax Trees (e.g., parsing strings into Python objects)\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # For data normalization and encoding categorical features\n",
    "from sklearn.decomposition import PCA  # For dimensionality reduction using Principal Component Analysis\n",
    "import cv2  # OpenCV library for image processing\n",
    "from tqdm import tqdm  # For creating progress bars\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import random  # Python module for generating random numbers\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "random.seed(42)  # Set the random seed for Python's random module\n",
    "np.random.seed(42)  # Set the random seed for NumPy\n",
    "torch.manual_seed(42)  # Set the random seed for PyTorch CPU operations\n",
    "torch.cuda.manual_seed(42)  # Set the random seed for PyTorch GPU operations (single device)\n",
    "torch.cuda.manual_seed_all(42)  # Set the random seed for PyTorch GPU operations (all devices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L59RNXykgy_n"
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Initialize preprocessing objects\n",
    "        self.cgm_scaler = None  # For scaling CGM data\n",
    "        self.demo_onehot_encoder = None  # For encoding categorical demographic data\n",
    "        self.demo_num_scaler = None  # For scaling numerical demographic data\n",
    "        self.demo_viome_scaler = None  # For scaling Viome data\n",
    "        self.demo_viome_pca = None  # For PCA on Viome data\n",
    "        self.image_scaler = None  # For scaling image data\n",
    "        self.label_scaler = None  # For scaling label data\n",
    "\n",
    "    def fit_preprocess_demo_data(self, demo_file, cgm_file):\n",
    "        # Load demographic and CGM data\n",
    "        demo_data = pd.read_csv(demo_file)\n",
    "        cgm_data = pd.read_csv(cgm_file)\n",
    "\n",
    "        # Define categorical and numerical columns\n",
    "        cat_columns = ['Gender', 'Race', 'Diabetes Status']\n",
    "        num_columns = ['Age', 'Weight', 'Height', 'A1C', 'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol', 'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI']\n",
    "\n",
    "        # One-hot encode categorical data\n",
    "        self.demo_onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded_cats = self.demo_onehot_encoder.fit_transform(demo_data[cat_columns])\n",
    "\n",
    "        # Scale numerical data\n",
    "        self.demo_num_scaler = StandardScaler()\n",
    "        normalized_nums = self.demo_num_scaler.fit_transform(demo_data[num_columns])\n",
    "\n",
    "        # Process Viome data\n",
    "        viome_data = demo_data['Viome'].apply(lambda x: np.array(x.split(','), dtype=float))\n",
    "        viome_matrix = np.vstack(viome_data.values)\n",
    "        self.demo_viome_scaler = StandardScaler()\n",
    "        viome_matrix_scaled = self.demo_viome_scaler.fit_transform(viome_matrix)\n",
    "\n",
    "        # Apply PCA to Viome data\n",
    "        self.demo_viome_pca = PCA(n_components=2)\n",
    "        viome_pca = self.demo_viome_pca.fit_transform(viome_matrix_scaled)\n",
    "\n",
    "        # Combine all processed features\n",
    "        processed_demo = np.hstack((encoded_cats, normalized_nums, viome_pca))\n",
    "\n",
    "        # Create a map of subject IDs to processed demographic data\n",
    "        subject_demo_map = {subject: data for subject, data in zip(demo_data['Subject ID'], processed_demo)}\n",
    "\n",
    "        # Replicate demographic data based on CGM data\n",
    "        replicated_demo = []\n",
    "        for _, row in cgm_data.iterrows():\n",
    "            subject_id = row['Subject ID']\n",
    "            replicated_demo.append(subject_demo_map[subject_id])\n",
    "\n",
    "        # Create column names for the processed data\n",
    "        columns = (self.demo_onehot_encoder.get_feature_names_out(cat_columns).tolist() + num_columns + ['PC1', 'PC2'])\n",
    "\n",
    "        # Create a DataFrame with the replicated data\n",
    "        replicated_demo_df = pd.DataFrame(replicated_demo, columns=columns)\n",
    "        replicated_demo_df.insert(0, 'Subject ID', cgm_data['Subject ID'])\n",
    "        replicated_demo_df.insert(1, 'Day', cgm_data['Day'])\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        replicated_demo_tensor = torch.FloatTensor(replicated_demo).unsqueeze(-1)\n",
    "        return replicated_demo_tensor\n",
    "\n",
    "    def transform_preprocess_demo_data(self, demo_file, cgm_file):\n",
    "        # Load demographic and CGM data\n",
    "        demo_data = pd.read_csv(demo_file)\n",
    "        cgm_data = pd.read_csv(cgm_file)\n",
    "\n",
    "        # Define categorical and numerical columns\n",
    "        cat_columns = ['Gender', 'Race', 'Diabetes Status']\n",
    "        num_columns = ['Age', 'Weight', 'Height', 'A1C', 'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol', 'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI']\n",
    "\n",
    "        # Transform categorical data using pre-fitted encoder\n",
    "        encoded_cats = self.demo_onehot_encoder.transform(demo_data[cat_columns])\n",
    "\n",
    "        # Transform numerical data using pre-fitted scaler\n",
    "        normalized_nums = self.demo_num_scaler.transform(demo_data[num_columns])\n",
    "\n",
    "        # Process Viome data\n",
    "        viome_data = demo_data['Viome'].apply(lambda x: np.array(x.split(','), dtype=float))\n",
    "        viome_matrix = np.vstack(viome_data.values)\n",
    "        viome_matrix_scaled = self.demo_viome_scaler.transform(viome_matrix)\n",
    "\n",
    "        # Apply pre-fitted PCA to Viome data\n",
    "        viome_pca = self.demo_viome_pca.transform(viome_matrix_scaled)\n",
    "\n",
    "        # Combine all processed features\n",
    "        processed_demo = np.hstack((encoded_cats, normalized_nums, viome_pca))\n",
    "\n",
    "        # Create a map of subject IDs to processed demographic data\n",
    "        subject_demo_map = {subject: data for subject, data in zip(demo_data['Subject ID'], processed_demo)}\n",
    "\n",
    "        # Replicate demographic data based on CGM data\n",
    "        replicated_demo = []\n",
    "        for _, row in cgm_data.iterrows():\n",
    "            subject_id = row['Subject ID']\n",
    "            replicated_demo.append(subject_demo_map[subject_id])\n",
    "\n",
    "        # Create column names for the processed data\n",
    "        columns = (self.demo_onehot_encoder.get_feature_names_out(cat_columns).tolist() + num_columns + ['PC1', 'PC2'])\n",
    "\n",
    "        # Create a DataFrame with the replicated data\n",
    "        replicated_demo_df = pd.DataFrame(replicated_demo, columns=columns)\n",
    "        replicated_demo_df.insert(0, 'Subject ID', cgm_data['Subject ID'])\n",
    "        replicated_demo_df.insert(1, 'Day', cgm_data['Day'])\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        replicated_demo_tensor = torch.FloatTensor(replicated_demo).unsqueeze(-1)\n",
    "        return replicated_demo_tensor\n",
    "\n",
    "    def fit_preprocess_labels(self, label_file):\n",
    "        # Load and preprocess label data\n",
    "        labels = pd.read_csv(label_file)\n",
    "        lunch_calories = labels['Lunch Calories'].values.reshape(-1, 1)\n",
    "\n",
    "        # Scale lunch calorie data\n",
    "        self.label_scaler = StandardScaler()\n",
    "        scaled_lunch_calories = self.label_scaler.fit_transform(lunch_calories)\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        lunch_calories_tensor = torch.FloatTensor(scaled_lunch_calories)\n",
    "        return lunch_calories_tensor\n",
    "\n",
    "    def fit_preprocess_cgm_data(self, cgm_file):\n",
    "        # Load CGM data\n",
    "        cgm_data = pd.read_csv(cgm_file)\n",
    "        feature_list = []\n",
    "\n",
    "        # Extract features from CGM time series data\n",
    "        for index, row in cgm_data.iterrows():\n",
    "            try:\n",
    "                # Parse CGM data and create time series\n",
    "                cgm_list = ast.literal_eval(row['CGM Data'])\n",
    "                time_values = [x[0] for x in cgm_list if x[1] is not None]\n",
    "                glucose_values = [float(x[1]) for x in cgm_list if x[1] is not None]\n",
    "                time_series = pd.Series(glucose_values, index=pd.to_datetime(time_values))\n",
    "\n",
    "                # Resample and interpolate time series\n",
    "                time_series = time_series.resample('5min').mean().interpolate('linear').fillna(0)\n",
    "\n",
    "                # Calculate various statistics\n",
    "                mean_glucose = time_series.mean()\n",
    "                std_glucose = time_series.std()\n",
    "                min_glucose = time_series.min()\n",
    "                max_glucose = time_series.max()\n",
    "                range_glucose = max_glucose - min_glucose\n",
    "\n",
    "                # Calculate pre-lunch statistics\n",
    "                pre_lunch_mean = (\n",
    "                    time_series[row['Breakfast Time']:row['Lunch Time']].mean()\n",
    "                    if row['Lunch Time'] in time_series.index else 0\n",
    "                )\n",
    "                pre_lunch_std = (\n",
    "                    time_series[row['Breakfast Time']:row['Lunch Time']].std()\n",
    "                    if row['Lunch Time'] in time_series.index else 0\n",
    "                )\n",
    "\n",
    "                # Calculate trend\n",
    "                trend = np.polyfit(range(len(time_series)), time_series, 1)[0]\n",
    "\n",
    "                feature_list.append([\n",
    "                    mean_glucose, std_glucose, min_glucose, max_glucose, range_glucose,\n",
    "                    pre_lunch_mean, pre_lunch_std, trend\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                # Handle errors by appending zeros\n",
    "                feature_list.append([0] * 8)\n",
    "\n",
    "        # Create DataFrame with extracted features\n",
    "        feature_df = pd.DataFrame(feature_list, columns=[\n",
    "            'Mean Glucose', 'Std Glucose', 'Min Glucose', 'Max Glucose', 'Range Glucose',\n",
    "            'Pre-Lunch Mean', 'Pre-Lunch Std', 'Trend'\n",
    "        ])\n",
    "\n",
    "        # Scale features\n",
    "        self.cgm_scaler = StandardScaler()\n",
    "        scaled_features = self.cgm_scaler.fit_transform(feature_df)\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        feature_tensor = torch.FloatTensor(scaled_features).unsqueeze(-1)\n",
    "        return feature_tensor\n",
    "\n",
    "    def transform_preprocess_cgm_data(self, cgm_file):\n",
    "        # Load CGM data\n",
    "        cgm_data = pd.read_csv(cgm_file)\n",
    "        feature_list = []\n",
    "\n",
    "        # Extract features from CGM time series data\n",
    "        for index, row in cgm_data.iterrows():\n",
    "            try:\n",
    "                # Parse CGM data and create time series\n",
    "                cgm_list = ast.literal_eval(row['CGM Data'])\n",
    "                time_values = [x[0] for x in cgm_list if x[1] is not None]\n",
    "                glucose_values = [float(x[1]) for x in cgm_list if x[1] is not None]\n",
    "                time_series = pd.Series(glucose_values, index=pd.to_datetime(time_values))\n",
    "\n",
    "                # Resample and interpolate time series\n",
    "                time_series = time_series.resample('5min').mean().interpolate('linear').fillna(0)\n",
    "\n",
    "                # Calculate various statistics\n",
    "                mean_glucose = time_series.mean()\n",
    "                std_glucose = time_series.std()\n",
    "                min_glucose = time_series.min()\n",
    "                max_glucose = time_series.max()\n",
    "                range_glucose = max_glucose - min_glucose\n",
    "\n",
    "                # Calculate pre-lunch statistics\n",
    "                pre_lunch_mean = (\n",
    "                    time_series[row['Breakfast Time']:row['Lunch Time']].mean()\n",
    "                    if row['Lunch Time'] in time_series.index else 0\n",
    "                )\n",
    "                pre_lunch_std = (\n",
    "                    time_series[row['Breakfast Time']:row['Lunch Time']].std()\n",
    "                    if row['Lunch Time'] in time_series.index else 0\n",
    "                )\n",
    "\n",
    "                # Calculate trend\n",
    "                trend = np.polyfit(range(len(time_series)), time_series, 1)[0]\n",
    "\n",
    "                feature_list.append([\n",
    "                    mean_glucose, std_glucose, min_glucose, max_glucose, range_glucose,\n",
    "                    pre_lunch_mean, pre_lunch_std, trend\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                # Handle errors by appending zeros\n",
    "                feature_list.append([0] * 8)\n",
    "\n",
    "        # Create DataFrame with extracted features\n",
    "        feature_df = pd.DataFrame(feature_list, columns=[\n",
    "            'Mean Glucose', 'Std Glucose', 'Min Glucose', 'Max Glucose', 'Range Glucose',\n",
    "            'Pre-Lunch Mean', 'Pre-Lunch Std', 'Trend'\n",
    "        ])\n",
    "\n",
    "        # Scale features using pre-fitted scaler\n",
    "        scaled_features = self.cgm_scaler.transform(feature_df)\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        feature_tensor = torch.FloatTensor(scaled_features).unsqueeze(-1)\n",
    "        return feature_tensor\n",
    "\n",
    "    def fit_preprocess_images(self, img_file, target_size=(64, 64)):\n",
    "        def process_single_image(img_data):\n",
    "            try:\n",
    "                # Convert image data to numpy array\n",
    "                img_array = np.array(ast.literal_eval(img_data), dtype=np.float32)\n",
    "\n",
    "                # Resize image if necessary\n",
    "                if img_array.shape != (*target_size, 3):\n",
    "                    if len(img_array.shape) == 3:\n",
    "                        img_array = cv2.resize(img_array, target_size)\n",
    "                    else:\n",
    "                        img_array = np.zeros((*target_size, 3), dtype=np.float32)\n",
    "\n",
    "                # Convert color space and apply Gaussian blur\n",
    "                img_array = cv2.cvtColor(img_array.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                img_array = cv2.GaussianBlur(img_array, (3,3), 0)\n",
    "                return img_array\n",
    "            except (ValueError, SyntaxError, TypeError):\n",
    "                # Return zero array if processing fails\n",
    "                return np.zeros((*target_size, 3), dtype=np.float32)\n",
    "\n",
    "        # Load image data\n",
    "        image_data = pd.read_csv(img_file)\n",
    "\n",
    "        # Process breakfast and lunch images\n",
    "        breakfast_images = [process_single_image(img) for img in image_data['Image Before Breakfast']]\n",
    "        lunch_images = [process_single_image(img) for img in image_data['Image Before Lunch']]\n",
    "\n",
    "        # Stack images into 4D arrays\n",
    "        breakfast_images = np.stack(breakfast_images)\n",
    "        lunch_images = np.stack(lunch_images)\n",
    "\n",
    "        # Fit scaler on combined image data\n",
    "        self.image_scaler = StandardScaler()\n",
    "        reshaped_breakfast = breakfast_images.reshape(-1, target_size[0] * target_size[1] * 3)\n",
    "        reshaped_lunch = lunch_images.reshape(-1, target_size[0] * target_size[1] * 3)\n",
    "        combined_images = np.vstack([reshaped_breakfast, reshaped_lunch])\n",
    "        self.image_scaler.fit(combined_images)\n",
    "\n",
    "        # Normalize pixel values\n",
    "        breakfast_images = breakfast_images / 255.0\n",
    "        lunch_images = lunch_images / 255.0\n",
    "\n",
    "        # Convert to PyTorch tensors and rearrange dimensions\n",
    "        breakfast_tensor = torch.FloatTensor(breakfast_images).permute(0, 3, 1, 2)\n",
    "        lunch_tensor = torch.FloatTensor(lunch_images).permute(0, 3, 1, 2)\n",
    "\n",
    "        return breakfast_tensor, lunch_tensor\n",
    "\n",
    "    def transform_preprocess_images(self, img_file, target_size=(64, 64)):\n",
    "        def process_single_image(img_data):\n",
    "            try:\n",
    "                # Convert string representation of image data to numpy array\n",
    "                img_array = np.array(ast.literal_eval(img_data), dtype=np.float32)\n",
    "\n",
    "                # Resize image if it doesn't match the target size\n",
    "                if img_array.shape != (*target_size, 3):\n",
    "                    if len(img_array.shape) == 3:\n",
    "                        img_array = cv2.resize(img_array, target_size)\n",
    "                    else:\n",
    "                        # Create a blank image if the shape is invalid\n",
    "                        img_array = np.zeros((*target_size, 3), dtype=np.float32)\n",
    "\n",
    "                # Convert color space from BGR to RGB\n",
    "                img_array = cv2.cvtColor(img_array.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "                # Apply Gaussian blur for noise reduction\n",
    "                img_array = cv2.GaussianBlur(img_array, (3,3), 0)\n",
    "\n",
    "                return img_array\n",
    "\n",
    "            except (ValueError, SyntaxError, TypeError):\n",
    "                # Return a blank image if processing fails\n",
    "                return np.zeros((*target_size, 3), dtype=np.float32)\n",
    "\n",
    "        # Load image data from CSV file\n",
    "        image_data = pd.read_csv(img_file)\n",
    "\n",
    "        # Process breakfast and lunch images\n",
    "        breakfast_images = [process_single_image(img) for img in image_data['Image Before Breakfast']]\n",
    "        lunch_images = [process_single_image(img) for img in image_data['Image Before Lunch']]\n",
    "\n",
    "        # Stack processed images into 3D arrays\n",
    "        breakfast_images = np.stack(breakfast_images)\n",
    "        lunch_images = np.stack(lunch_images)\n",
    "\n",
    "        # Normalize pixel values to [0, 1] range\n",
    "        breakfast_images = breakfast_images / 255.0\n",
    "        lunch_images = lunch_images / 255.0\n",
    "\n",
    "        # Convert to PyTorch tensors and rearrange dimensions (N, H, W, C) -> (N, C, H, W)\n",
    "        breakfast_tensor = torch.FloatTensor(breakfast_images).permute(0, 3, 1, 2)\n",
    "        lunch_tensor = torch.FloatTensor(lunch_images).permute(0, 3, 1, 2)\n",
    "\n",
    "        return breakfast_tensor, lunch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eudr9-fPg47N"
   },
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, cgm_file, demo_file, img_file, label_file=None, transform=None, preprocessor=None, is_train=True):\n",
    "        # Initialize the dataset with various data files and preprocessing options\n",
    "        if preprocessor:\n",
    "            if is_train:\n",
    "                # For training data, fit and preprocess the data\n",
    "                self.cgm_data = preprocessor.fit_preprocess_cgm_data(cgm_file)\n",
    "                self.demo_data = preprocessor.fit_preprocess_demo_data(demo_file, cgm_file)\n",
    "                self.breakfast_images, self.lunch_images = preprocessor.fit_preprocess_images(img_file)\n",
    "\n",
    "                if label_file:\n",
    "                    # Preprocess labels if provided\n",
    "                    self.labels = preprocessor.fit_preprocess_labels(label_file)\n",
    "            else:\n",
    "                # For non-training data, transform the data using pre-fitted preprocessors\n",
    "                self.cgm_data = preprocessor.transform_preprocess_cgm_data(cgm_file)\n",
    "                self.demo_data = preprocessor.transform_preprocess_demo_data(demo_file, cgm_file)\n",
    "                self.breakfast_images, self.lunch_images = preprocessor.transform_preprocess_images(img_file)\n",
    "\n",
    "        # Store the transform function for later use\n",
    "        self.transform = transform\n",
    "        # Flag to indicate whether labels are available\n",
    "        self.has_labels = label_file is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.cgm_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a single sample from the dataset\n",
    "        sample = {\n",
    "            'cgm': self.cgm_data[idx],\n",
    "            'demo': self.demo_data[idx],\n",
    "            'breakfast_img': self.breakfast_images[idx],\n",
    "            'lunch_img': self.lunch_images[idx]\n",
    "        }\n",
    "\n",
    "        # Apply transforms to the images if specified\n",
    "        if self.transform:\n",
    "            sample['breakfast_img'] = self.transform(sample['breakfast_img'])\n",
    "            sample['lunch_img'] = self.transform(sample['lunch_img'])\n",
    "\n",
    "        # Include label in the sample if available\n",
    "        if self.has_labels:\n",
    "            sample['label'] = self.labels[idx]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JD3f4a5qg7Yi"
   },
   "outputs": [],
   "source": [
    "class CGMEncoder(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=32):\n",
    "        super(CGMEncoder, self).__init__()\n",
    "        # LSTM layer for processing CGM time series data\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,  # Number of features in input sequence\n",
    "            hidden_size=hidden_size,  # Number of features in hidden state\n",
    "            num_layers=num_layers,  # Number of recurrent layers\n",
    "            batch_first=True,  # Input shape is (batch, seq, feature)\n",
    "            dropout=0.2  # Dropout rate between LSTM layers\n",
    "        )\n",
    "        # Fully connected layer to produce final output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input through LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use only the last output of the sequence\n",
    "        # Apply fully connected layer to get final output\n",
    "        return self.fc(lstm_out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_u2m1UIug9ax"
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, output_size=32):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        # Convolutional layers for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Input: 3 channels (RGB), Output: 32 feature maps\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.BatchNorm2d(32),  # Batch normalization for faster convergence\n",
    "            nn.MaxPool2d(2),  # Reduce spatial dimensions by half\n",
    "\n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Input: 32 channels, Output: 64 feature maps\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Input: 64 channels, Output: 128 feature maps\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Fourth convolutional block\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # Input: 128 channels, Output: 256 feature maps\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),  # Regularization to prevent overfitting\n",
    "            nn.Linear(256 * 4 * 4, 512),  # Flatten and reduce dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, output_size)  # Final layer to produce desired output size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Pass input through convolutional layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for fully connected layers\n",
    "        x = self.classifier(x)  # Pass through fully connected layers\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P5XdvUK9g_vQ"
   },
   "outputs": [],
   "source": [
    "class DemoEncoder(nn.Module):\n",
    "    def __init__(self, input_size=25, output_size=32):\n",
    "        super(DemoEncoder, self).__init__()\n",
    "        # Define the encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),  # First fully connected layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.BatchNorm1d(64),  # Batch normalization for faster convergence\n",
    "            nn.Linear(64, output_size)  # Second fully connected layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(-1)  # Remove the last dimension if it's 1\n",
    "        return self.encoder(x)  # Pass input through the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fm9ZViOmhBAD"
   },
   "outputs": [],
   "source": [
    "class ModalityEmbedding(nn.Module):\n",
    "    def __init__(self, encoding_dim=32, embedding_dim=128):\n",
    "        super(ModalityEmbedding, self).__init__()\n",
    "        # Define the embedding layer\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, embedding_dim),  # Linear transformation from encoding_dim to embedding_dim\n",
    "            nn.LayerNorm(embedding_dim),  # Layer normalization for stability\n",
    "            nn.ReLU()  # ReLU activation function\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # Apply the embedding transformation to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xUpJrdqmhCh9"
   },
   "outputs": [],
   "source": [
    "class MultimodalAttentionFusion(nn.Module):\n",
    "    def __init__(self, encoding_dim=32, embedding_dim=128, num_heads=4, dropout=0.1):\n",
    "        super(MultimodalAttentionFusion, self).__init__()\n",
    "        # Embedding layers for each modality\n",
    "        self.cgm_embedding = ModalityEmbedding(encoding_dim, embedding_dim)\n",
    "        self.demo_embedding = ModalityEmbedding(encoding_dim, embedding_dim)\n",
    "        self.image_embedding = ModalityEmbedding(encoding_dim, embedding_dim)\n",
    "        \n",
    "        # Multi-head attention layer for fusion\n",
    "        self.multihead_attn = nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
    "        \n",
    "        # Layer normalization for attention output\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        # Layer normalization for feed-forward output\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Final predictor layer\n",
    "        self.predictor = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, cgm_encoded, demo_encoded, breakfast_encoded, lunch_encoded):\n",
    "        # Embed each modality\n",
    "        cgm_embed = self.cgm_embedding(cgm_encoded)\n",
    "        demo_embed = self.demo_embedding(demo_encoded)\n",
    "        breakfast_embed = self.image_embedding(breakfast_encoded)\n",
    "        lunch_embed = self.image_embedding(lunch_encoded)\n",
    "        \n",
    "        # Stack embeddings for attention\n",
    "        embeddings = torch.stack([cgm_embed, demo_embed, breakfast_embed, lunch_embed])\n",
    "        \n",
    "        # Apply multi-head attention\n",
    "        attn_output, _ = self.multihead_attn(embeddings, embeddings, embeddings)\n",
    "        \n",
    "        # Add & Norm (residual connection and layer normalization)\n",
    "        attn_output = self.norm1(attn_output + embeddings)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(attn_output)\n",
    "        \n",
    "        # Add & Norm (residual connection and layer normalization)\n",
    "        output = self.norm2(ffn_output + attn_output)\n",
    "        \n",
    "        # Average pooling across modalities\n",
    "        fused_embedding = torch.mean(output, dim=0)\n",
    "        \n",
    "        # Final prediction\n",
    "        lunch_calories = self.predictor(fused_embedding)\n",
    "        \n",
    "        return lunch_calories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gX4r4lz5hIMM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_7604\\3498538583.py:135: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  time_series = pd.Series(glucose_values, index=pd.to_datetime(time_values))\n",
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_7604\\3498538583.py:135: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  time_series = pd.Series(glucose_values, index=pd.to_datetime(time_values))\n",
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_7604\\3498538583.py:135: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  time_series = pd.Series(glucose_values, index=pd.to_datetime(time_values))\n",
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_7604\\3498538583.py:135: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  time_series = pd.Series(glucose_values, index=pd.to_datetime(time_values))\n",
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_7604\\3498538583.py:135: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  time_series = pd.Series(glucose_values, index=pd.to_datetime(time_values))\n",
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_7604\\3498538583.py:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  replicated_demo_tensor = torch.FloatTensor(replicated_demo).unsqueeze(-1)\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize encoder models and move them to the selected device\n",
    "cgm_encoder = CGMEncoder().to(device)\n",
    "image_encoder = ImageEncoder().to(device)\n",
    "demo_encoder = DemoEncoder().to(device)\n",
    "multimodal_model = MultimodalAttentionFusion().to(device)\n",
    "\n",
    "# Create a data preprocessor instance\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = MultimodalDataset(\n",
    "    cgm_file='cgm_train.csv',\n",
    "    demo_file='demo_viome_train.csv',\n",
    "    img_file='img_train.csv',\n",
    "    label_file='label_train.csv',\n",
    "    preprocessor=preprocessor,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "# Create testing dataset\n",
    "test_dataset = MultimodalDataset(\n",
    "    cgm_file='cgm_test.csv',\n",
    "    demo_file='demo_viome_test.csv',\n",
    "    img_file='img_test.csv',\n",
    "    preprocessor=preprocessor,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# Split training dataset into training and validation subsets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize Adam optimizer for all model parameters\n",
    "optimizer = optim.Adam(\n",
    "    list(cgm_encoder.parameters()) +\n",
    "    list(image_encoder.parameters()) +\n",
    "    list(demo_encoder.parameters()) +\n",
    "    list(multimodal_model.parameters()),\n",
    "    lr=0.0001\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Define custom loss function (Root Mean Squared Relative Error)\n",
    "def custom_loss(solution, submission):\n",
    "    rmsre = torch.sqrt(torch.mean(((solution - submission) / solution) ** 2))\n",
    "    return rmsre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4KudabwQhK4o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.0285, Val Loss: 1.1354\n",
      "Epoch [2/50], Train Loss: 0.9558, Val Loss: 0.8958\n",
      "Epoch [3/50], Train Loss: 0.8489, Val Loss: 0.7645\n",
      "Epoch [4/50], Train Loss: 0.8405, Val Loss: 0.7485\n",
      "Epoch [5/50], Train Loss: 0.8335, Val Loss: 0.7591\n",
      "Epoch [6/50], Train Loss: 0.8024, Val Loss: 0.7160\n",
      "Epoch [7/50], Train Loss: 0.8479, Val Loss: 0.7466\n",
      "Epoch [8/50], Train Loss: 0.8383, Val Loss: 0.7165\n",
      "Epoch [9/50], Train Loss: 0.7791, Val Loss: 0.6988\n",
      "Epoch [10/50], Train Loss: 0.8304, Val Loss: 0.6919\n",
      "Epoch [11/50], Train Loss: 0.7701, Val Loss: 0.7071\n",
      "Epoch [12/50], Train Loss: 0.7767, Val Loss: 0.7237\n",
      "Epoch [13/50], Train Loss: 0.7298, Val Loss: 0.7360\n",
      "Epoch [14/50], Train Loss: 0.7162, Val Loss: 0.7876\n",
      "Early stopping triggered at epoch 15\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "base_val_loss = float('inf')\n",
    "base_model_state = None\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "num_epochs = 50\n",
    "\n",
    "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set models to training mode\n",
    "    cgm_encoder.train()\n",
    "    image_encoder.train()\n",
    "    demo_encoder.train()\n",
    "    multimodal_model.train()\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for batch in train_loader:\n",
    "        # Move batch data to device\n",
    "        cgm_data = batch['cgm'].to(device)\n",
    "        demo_data = batch['demo'].to(device)\n",
    "        breakfast_img = batch['breakfast_img'].to(device)\n",
    "        lunch_img = batch['lunch_img'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass through encoders\n",
    "        cgm_encoded = cgm_encoder(cgm_data)\n",
    "        demo_encoded = demo_encoder(demo_data)\n",
    "        breakfast_encoded = image_encoder(breakfast_img)\n",
    "        lunch_encoded = image_encoder(lunch_img)\n",
    "\n",
    "        # Forward pass through multimodal model\n",
    "        predictions = multimodal_model(\n",
    "            cgm_encoded,\n",
    "            demo_encoded,\n",
    "            breakfast_encoded,\n",
    "            lunch_encoded\n",
    "        )\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = custom_loss(labels, predictions)\n",
    "        epoch_train_losses.append(loss.item())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(cgm_encoder.parameters()) +\n",
    "            list(image_encoder.parameters()) +\n",
    "            list(demo_encoder.parameters()) +\n",
    "            list(multimodal_model.parameters()),\n",
    "            max_norm=1.0\n",
    "        )\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    cgm_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "    demo_encoder.eval()\n",
    "    multimodal_model.eval()\n",
    "\n",
    "    epoch_val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Similar forward pass as in training, but without gradient calculation\n",
    "            cgm_data = batch['cgm'].to(device)\n",
    "            demo_data = batch['demo'].to(device)\n",
    "            breakfast_img = batch['breakfast_img'].to(device)\n",
    "            lunch_img = batch['lunch_img'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            cgm_encoded = cgm_encoder(cgm_data)\n",
    "            demo_encoded = demo_encoder(demo_data)\n",
    "            breakfast_encoded = image_encoder(breakfast_img)\n",
    "            lunch_encoded = image_encoder(lunch_img)\n",
    "\n",
    "            predictions = multimodal_model(\n",
    "                cgm_encoded,\n",
    "                demo_encoded,\n",
    "                breakfast_encoded,\n",
    "                lunch_encoded\n",
    "            )\n",
    "\n",
    "            val_loss = custom_loss(labels, predictions)\n",
    "            epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_val_loss = sum(epoch_val_losses) / len(epoch_val_losses)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Update learning rate based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping and model checkpointing\n",
    "    if avg_val_loss < base_val_loss:\n",
    "        base_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        base_model_state = {\n",
    "            'cgm_encoder': cgm_encoder.state_dict(),\n",
    "            'image_encoder': image_encoder.state_dict(),\n",
    "            'demo_encoder': demo_encoder.state_dict(),\n",
    "            'multimodal_model': multimodal_model.state_dict()\n",
    "        }\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train RMSRE')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation RMSRE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSRE Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LTP06l0l58lX"
   },
   "outputs": [],
   "source": [
    "def random_search(train_dataset, device, n_trials):\n",
    "    import random\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss\n",
    "    best_params = None  # Store best hyperparameters\n",
    "    best_model_state = None  # Store best model state\n",
    "    results = []  # List to store results of all trials\n",
    "\n",
    "    # Define the hyperparameter search space\n",
    "    param_space = {\n",
    "        'lstm_hidden_size': [32, 64, 128],\n",
    "        'lstm_layers': [1, 2],\n",
    "        'encoding_dim': [32, 64],\n",
    "        'embedding_dim': [64, 128],\n",
    "        'num_heads': [4, 8],\n",
    "        'dropout': [0.1, 0.2, 0.3],\n",
    "        'batch_size': [16, 32],\n",
    "        'learning_rate': [1e-3, 1e-4]\n",
    "    }\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # Randomly select hyperparameters for this trial\n",
    "        params = {k: random.choice(v) for k, v in param_space.items()}\n",
    "\n",
    "        print(f\"\\nTrial {trial+1}/{n_trials}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "\n",
    "        # Split dataset into train and validation subsets\n",
    "        train_size = int(0.8 * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        # Initialize model components with selected hyperparameters\n",
    "        cgm_encoder = CGMEncoder(\n",
    "            hidden_size=params['lstm_hidden_size'],\n",
    "            num_layers=params['lstm_layers'],\n",
    "            output_size=params['encoding_dim']\n",
    "        ).to(device)\n",
    "\n",
    "        image_encoder = ImageEncoder(output_size=params['encoding_dim']).to(device)\n",
    "        demo_encoder = DemoEncoder(output_size=params['encoding_dim']).to(device)\n",
    "\n",
    "        multimodal_model = MultimodalAttentionFusion(\n",
    "            encoding_dim=params['encoding_dim'],\n",
    "            embedding_dim=params['embedding_dim'],\n",
    "            num_heads=params['num_heads'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(\n",
    "            list(cgm_encoder.parameters()) +\n",
    "            list(image_encoder.parameters()) +\n",
    "            list(demo_encoder.parameters()) +\n",
    "            list(multimodal_model.parameters()),\n",
    "            lr=params['learning_rate']\n",
    "        )\n",
    "\n",
    "        # Initialize learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        best_trial_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(50):\n",
    "            # Set models to training mode\n",
    "            cgm_encoder.train()\n",
    "            image_encoder.train()\n",
    "            demo_encoder.train()\n",
    "            multimodal_model.train()\n",
    "\n",
    "            epoch_train_losses = []\n",
    "            for batch in train_loader:\n",
    "                # Move batch data to device\n",
    "                cgm_data = batch['cgm'].to(device)\n",
    "                demo_data = batch['demo'].to(device)\n",
    "                breakfast_img = batch['breakfast_img'].to(device)\n",
    "                lunch_img = batch['lunch_img'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                # Forward pass through encoders\n",
    "                cgm_encoded = cgm_encoder(cgm_data)\n",
    "                demo_encoded = demo_encoder(demo_data)\n",
    "                breakfast_encoded = image_encoder(breakfast_img)\n",
    "                lunch_encoded = image_encoder(lunch_img)\n",
    "\n",
    "                # Forward pass through multimodal model\n",
    "                predictions = multimodal_model(\n",
    "                    cgm_encoded,\n",
    "                    demo_encoded,\n",
    "                    breakfast_encoded,\n",
    "                    lunch_encoded\n",
    "                )\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = custom_loss(labels, predictions)\n",
    "                epoch_train_losses.append(loss.item())\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Calculate average training loss for the epoch\n",
    "            avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "\n",
    "            # Validation phase\n",
    "            cgm_encoder.eval()\n",
    "            image_encoder.eval()\n",
    "            demo_encoder.eval()\n",
    "            multimodal_model.eval()\n",
    "\n",
    "            epoch_val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    # Similar forward pass as in training, but without gradient calculation\n",
    "                    cgm_data = batch['cgm'].to(device)\n",
    "                    demo_data = batch['demo'].to(device)\n",
    "                    breakfast_img = batch['breakfast_img'].to(device)\n",
    "                    lunch_img = batch['lunch_img'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    cgm_encoded = cgm_encoder(cgm_data)\n",
    "                    demo_encoded = demo_encoder(demo_data)\n",
    "                    breakfast_encoded = image_encoder(breakfast_img)\n",
    "                    lunch_encoded = image_encoder(lunch_img)\n",
    "\n",
    "                    predictions = multimodal_model(\n",
    "                        cgm_encoded,\n",
    "                        demo_encoded,\n",
    "                        breakfast_encoded,\n",
    "                        lunch_encoded\n",
    "                    )\n",
    "\n",
    "                    val_loss = custom_loss(labels, predictions)\n",
    "                    epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "            # Calculate average validation loss for the epoch\n",
    "            avg_val_loss = sum(epoch_val_losses) / len(epoch_val_losses)\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/50], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "            # Check if current model is the best so far\n",
    "            if avg_val_loss < best_trial_val_loss:\n",
    "                best_trial_val_loss = avg_val_loss\n",
    "                if best_trial_val_loss < best_val_loss:\n",
    "                    best_val_loss = best_trial_val_loss\n",
    "                    best_params = params\n",
    "                    best_model_state = {\n",
    "                        'cgm_encoder': cgm_encoder.state_dict(),\n",
    "                        'image_encoder': image_encoder.state_dict(),\n",
    "                        'demo_encoder': demo_encoder.state_dict(),\n",
    "                        'multimodal_model': multimodal_model.state_dict()\n",
    "                    }\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "        # Store results for this trial\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'val_loss': best_trial_val_loss\n",
    "        })\n",
    "\n",
    "    return best_params, best_model_state, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SRdk5wGQ-ak3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 1, 'encoding_dim': 32, 'embedding_dim': 128, 'num_heads': 4, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.2366, Val Loss: 1.0285\n",
      "Epoch [2/50], Train Loss: 0.9145, Val Loss: 0.9040\n",
      "Epoch [3/50], Train Loss: 0.9056, Val Loss: 0.9197\n",
      "Epoch [4/50], Train Loss: 0.8630, Val Loss: 1.0122\n",
      "Epoch [5/50], Train Loss: 0.8704, Val Loss: 0.9786\n",
      "Epoch [6/50], Train Loss: 0.9119, Val Loss: 1.2703\n",
      "Epoch [7/50], Train Loss: 0.9346, Val Loss: 1.0124\n",
      "Epoch [8/50], Train Loss: 0.8683, Val Loss: 0.9750\n",
      "Epoch [9/50], Train Loss: 0.8420, Val Loss: 0.9651\n",
      "Epoch [10/50], Train Loss: 0.8838, Val Loss: 0.9642\n",
      "Epoch [11/50], Train Loss: 0.8162, Val Loss: 0.9689\n",
      "Epoch [12/50], Train Loss: 0.8273, Val Loss: 0.9677\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 2/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 1, 'encoding_dim': 64, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 1.2920, Val Loss: 0.9585\n",
      "Epoch [2/50], Train Loss: 0.9285, Val Loss: 0.9315\n",
      "Epoch [3/50], Train Loss: 0.8856, Val Loss: 0.9406\n",
      "Epoch [4/50], Train Loss: 0.8825, Val Loss: 0.9148\n",
      "Epoch [5/50], Train Loss: 0.8570, Val Loss: 0.9040\n",
      "Epoch [6/50], Train Loss: 0.8798, Val Loss: 0.9191\n",
      "Epoch [7/50], Train Loss: 0.8683, Val Loss: 0.9442\n",
      "Epoch [8/50], Train Loss: 0.8804, Val Loss: 0.9429\n",
      "Epoch [9/50], Train Loss: 0.8584, Val Loss: 0.9226\n",
      "Epoch [10/50], Train Loss: 0.8581, Val Loss: 0.9214\n",
      "Epoch [11/50], Train Loss: 0.8553, Val Loss: 0.9249\n",
      "Epoch [12/50], Train Loss: 0.8452, Val Loss: 0.9627\n",
      "Epoch [13/50], Train Loss: 0.8228, Val Loss: 0.9621\n",
      "Epoch [14/50], Train Loss: 0.8324, Val Loss: 0.9659\n",
      "Epoch [15/50], Train Loss: 0.8018, Val Loss: 1.0008\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 3/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 1, 'encoding_dim': 32, 'embedding_dim': 128, 'num_heads': 4, 'dropout': 0.2, 'batch_size': 32, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 2.0333, Val Loss: 1.5537\n",
      "Epoch [2/50], Train Loss: 1.0895, Val Loss: 0.6883\n",
      "Epoch [3/50], Train Loss: 0.9068, Val Loss: 0.7739\n",
      "Epoch [4/50], Train Loss: 0.8681, Val Loss: 0.6706\n",
      "Epoch [5/50], Train Loss: 0.8966, Val Loss: 0.8533\n",
      "Epoch [6/50], Train Loss: 0.8652, Val Loss: 0.7075\n",
      "Epoch [7/50], Train Loss: 0.8125, Val Loss: 0.8353\n",
      "Epoch [8/50], Train Loss: 0.8738, Val Loss: 0.6779\n",
      "Epoch [9/50], Train Loss: 0.8390, Val Loss: 0.8392\n",
      "Epoch [10/50], Train Loss: 0.8050, Val Loss: 0.7360\n",
      "Epoch [11/50], Train Loss: 0.8425, Val Loss: 0.9615\n",
      "Epoch [12/50], Train Loss: 0.8574, Val Loss: 0.7655\n",
      "Epoch [13/50], Train Loss: 0.8426, Val Loss: 0.9142\n",
      "Epoch [14/50], Train Loss: 0.7629, Val Loss: 0.8490\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 4/20\n",
      "Parameters: {'lstm_hidden_size': 32, 'lstm_layers': 2, 'encoding_dim': 64, 'embedding_dim': 128, 'num_heads': 4, 'dropout': 0.1, 'batch_size': 32, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 1.8883, Val Loss: 1.2395\n",
      "Epoch [2/50], Train Loss: 0.9856, Val Loss: 1.0146\n",
      "Epoch [3/50], Train Loss: 0.8941, Val Loss: 1.2123\n",
      "Epoch [4/50], Train Loss: 1.0033, Val Loss: 0.9760\n",
      "Epoch [5/50], Train Loss: 0.9221, Val Loss: 0.9766\n",
      "Epoch [6/50], Train Loss: 0.9291, Val Loss: 0.9706\n",
      "Epoch [7/50], Train Loss: 0.9175, Val Loss: 1.0839\n",
      "Epoch [8/50], Train Loss: 0.9079, Val Loss: 1.0067\n",
      "Epoch [9/50], Train Loss: 0.8763, Val Loss: 0.9616\n",
      "Epoch [10/50], Train Loss: 0.8562, Val Loss: 0.9745\n",
      "Epoch [11/50], Train Loss: 0.8326, Val Loss: 0.9790\n",
      "Epoch [12/50], Train Loss: 0.8446, Val Loss: 0.9797\n",
      "Epoch [13/50], Train Loss: 0.8710, Val Loss: 0.9928\n",
      "Epoch [14/50], Train Loss: 0.8381, Val Loss: 1.0221\n",
      "Epoch [15/50], Train Loss: 0.8276, Val Loss: 1.0549\n",
      "Epoch [16/50], Train Loss: 0.8268, Val Loss: 1.0393\n",
      "Epoch [17/50], Train Loss: 0.8635, Val Loss: 1.0348\n",
      "Epoch [18/50], Train Loss: 0.8400, Val Loss: 1.0608\n",
      "Epoch [19/50], Train Loss: 0.7617, Val Loss: 1.0990\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 5/20\n",
      "Parameters: {'lstm_hidden_size': 32, 'lstm_layers': 2, 'encoding_dim': 32, 'embedding_dim': 128, 'num_heads': 8, 'dropout': 0.3, 'batch_size': 32, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 1.6058, Val Loss: 1.5396\n",
      "Epoch [2/50], Train Loss: 0.9822, Val Loss: 0.8568\n",
      "Epoch [3/50], Train Loss: 0.9257, Val Loss: 1.0691\n",
      "Epoch [4/50], Train Loss: 0.9156, Val Loss: 0.7588\n",
      "Epoch [5/50], Train Loss: 0.9194, Val Loss: 0.7615\n",
      "Epoch [6/50], Train Loss: 0.9195, Val Loss: 0.8244\n",
      "Epoch [7/50], Train Loss: 0.8678, Val Loss: 0.8851\n",
      "Epoch [8/50], Train Loss: 0.8936, Val Loss: 0.7647\n",
      "Epoch [9/50], Train Loss: 0.8427, Val Loss: 0.6658\n",
      "Epoch [10/50], Train Loss: 0.8673, Val Loss: 0.7032\n",
      "Epoch [11/50], Train Loss: 0.8288, Val Loss: 0.6534\n",
      "Epoch [12/50], Train Loss: 0.8630, Val Loss: 0.6752\n",
      "Epoch [13/50], Train Loss: 0.8330, Val Loss: 0.7332\n",
      "Epoch [14/50], Train Loss: 0.8419, Val Loss: 0.7629\n",
      "Epoch [15/50], Train Loss: 0.8505, Val Loss: 0.6647\n",
      "Epoch [16/50], Train Loss: 0.8469, Val Loss: 0.7757\n",
      "Epoch [17/50], Train Loss: 0.8257, Val Loss: 0.6144\n",
      "Epoch [18/50], Train Loss: 0.8474, Val Loss: 0.6923\n",
      "Epoch [19/50], Train Loss: 0.8663, Val Loss: 0.6541\n",
      "Epoch [20/50], Train Loss: 0.8315, Val Loss: 0.7359\n",
      "Epoch [21/50], Train Loss: 0.8284, Val Loss: 0.6604\n",
      "Epoch [22/50], Train Loss: 0.8450, Val Loss: 0.6758\n",
      "Epoch [23/50], Train Loss: 0.8549, Val Loss: 0.6763\n",
      "Epoch [24/50], Train Loss: 0.8229, Val Loss: 0.6860\n",
      "Epoch [25/50], Train Loss: 0.7862, Val Loss: 0.7733\n",
      "Epoch [26/50], Train Loss: 0.8140, Val Loss: 0.7420\n",
      "Epoch [27/50], Train Loss: 0.8096, Val Loss: 0.7135\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 6/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 2, 'encoding_dim': 32, 'embedding_dim': 128, 'num_heads': 4, 'dropout': 0.3, 'batch_size': 32, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 1.0606, Val Loss: 0.9335\n",
      "Epoch [2/50], Train Loss: 0.9141, Val Loss: 1.1620\n",
      "Epoch [3/50], Train Loss: 0.9206, Val Loss: 1.0645\n",
      "Epoch [4/50], Train Loss: 0.8772, Val Loss: 0.9310\n",
      "Epoch [5/50], Train Loss: 0.8933, Val Loss: 0.9578\n",
      "Epoch [6/50], Train Loss: 0.8312, Val Loss: 0.8959\n",
      "Epoch [7/50], Train Loss: 0.8598, Val Loss: 0.9476\n",
      "Epoch [8/50], Train Loss: 0.8616, Val Loss: 0.9055\n",
      "Epoch [9/50], Train Loss: 0.8689, Val Loss: 0.9251\n",
      "Epoch [10/50], Train Loss: 0.8321, Val Loss: 0.9324\n",
      "Epoch [11/50], Train Loss: 0.8312, Val Loss: 0.9207\n",
      "Epoch [12/50], Train Loss: 0.7705, Val Loss: 0.9043\n",
      "Epoch [13/50], Train Loss: 0.8287, Val Loss: 0.9197\n",
      "Epoch [14/50], Train Loss: 0.8172, Val Loss: 0.9259\n",
      "Epoch [15/50], Train Loss: 0.7841, Val Loss: 0.9215\n",
      "Epoch [16/50], Train Loss: 0.8353, Val Loss: 0.9307\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 7/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 1, 'encoding_dim': 32, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.2, 'batch_size': 16, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 0.9582, Val Loss: 0.9649\n",
      "Epoch [2/50], Train Loss: 0.8900, Val Loss: 0.9696\n",
      "Epoch [3/50], Train Loss: 0.8440, Val Loss: 1.0668\n",
      "Epoch [4/50], Train Loss: 0.8595, Val Loss: 1.0110\n",
      "Epoch [5/50], Train Loss: 0.8448, Val Loss: 1.2393\n",
      "Epoch [6/50], Train Loss: 0.8090, Val Loss: 1.0561\n",
      "Epoch [7/50], Train Loss: 0.7802, Val Loss: 1.1139\n",
      "Epoch [8/50], Train Loss: 0.7864, Val Loss: 1.0655\n",
      "Epoch [9/50], Train Loss: 0.7820, Val Loss: 1.0644\n",
      "Epoch [10/50], Train Loss: 0.7719, Val Loss: 1.1617\n",
      "Epoch [11/50], Train Loss: 0.7617, Val Loss: 1.0837\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 8/20\n",
      "Parameters: {'lstm_hidden_size': 32, 'lstm_layers': 2, 'encoding_dim': 64, 'embedding_dim': 128, 'num_heads': 8, 'dropout': 0.1, 'batch_size': 32, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 1.0219, Val Loss: 1.0786\n",
      "Epoch [2/50], Train Loss: 0.9307, Val Loss: 0.9518\n",
      "Epoch [3/50], Train Loss: 0.8902, Val Loss: 0.9556\n",
      "Epoch [4/50], Train Loss: 0.8663, Val Loss: 0.9609\n",
      "Epoch [5/50], Train Loss: 0.8977, Val Loss: 1.0177\n",
      "Epoch [6/50], Train Loss: 0.8746, Val Loss: 0.9874\n",
      "Epoch [7/50], Train Loss: 0.8815, Val Loss: 0.9630\n",
      "Epoch [8/50], Train Loss: 0.8439, Val Loss: 1.0124\n",
      "Epoch [9/50], Train Loss: 0.8598, Val Loss: 0.9712\n",
      "Epoch [10/50], Train Loss: 0.8457, Val Loss: 0.9793\n",
      "Epoch [11/50], Train Loss: 0.8290, Val Loss: 0.9825\n",
      "Epoch [12/50], Train Loss: 0.8930, Val Loss: 0.9779\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 9/20\n",
      "Parameters: {'lstm_hidden_size': 32, 'lstm_layers': 2, 'encoding_dim': 32, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.1, 'batch_size': 32, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 1.1897, Val Loss: 3.8519\n",
      "Epoch [2/50], Train Loss: 0.8380, Val Loss: 0.7371\n",
      "Epoch [3/50], Train Loss: 0.9211, Val Loss: 0.7750\n",
      "Epoch [4/50], Train Loss: 0.9251, Val Loss: 0.8407\n",
      "Epoch [5/50], Train Loss: 0.8881, Val Loss: 0.6180\n",
      "Epoch [6/50], Train Loss: 0.8526, Val Loss: 0.8530\n",
      "Epoch [7/50], Train Loss: 0.9072, Val Loss: 0.6165\n",
      "Epoch [8/50], Train Loss: 0.8890, Val Loss: 0.6037\n",
      "Epoch [9/50], Train Loss: 0.8614, Val Loss: 0.7240\n",
      "Epoch [10/50], Train Loss: 0.8273, Val Loss: 0.6303\n",
      "Epoch [11/50], Train Loss: 0.8216, Val Loss: 0.7234\n",
      "Epoch [12/50], Train Loss: 0.8420, Val Loss: 0.6422\n",
      "Epoch [13/50], Train Loss: 0.8826, Val Loss: 0.6053\n",
      "Epoch [14/50], Train Loss: 0.8163, Val Loss: 0.5970\n",
      "Epoch [15/50], Train Loss: 0.8512, Val Loss: 0.6701\n",
      "Epoch [16/50], Train Loss: 0.8169, Val Loss: 0.6189\n",
      "Epoch [17/50], Train Loss: 0.8115, Val Loss: 0.7585\n",
      "Epoch [18/50], Train Loss: 0.8577, Val Loss: 0.7344\n",
      "Epoch [19/50], Train Loss: 0.7705, Val Loss: 0.6191\n",
      "Epoch [20/50], Train Loss: 0.7833, Val Loss: 0.6457\n",
      "Epoch [21/50], Train Loss: 0.8132, Val Loss: 0.6971\n",
      "Epoch [22/50], Train Loss: 0.8059, Val Loss: 0.6503\n",
      "Epoch [23/50], Train Loss: 0.8211, Val Loss: 0.6968\n",
      "Epoch [24/50], Train Loss: 0.7851, Val Loss: 0.6323\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 10/20\n",
      "Parameters: {'lstm_hidden_size': 64, 'lstm_layers': 1, 'encoding_dim': 64, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.1, 'batch_size': 32, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 1.0515, Val Loss: 2.0333\n",
      "Epoch [2/50], Train Loss: 0.9306, Val Loss: 1.2057\n",
      "Epoch [3/50], Train Loss: 0.9347, Val Loss: 1.0034\n",
      "Epoch [4/50], Train Loss: 0.8553, Val Loss: 0.9274\n",
      "Epoch [5/50], Train Loss: 0.8482, Val Loss: 0.9206\n",
      "Epoch [6/50], Train Loss: 0.8180, Val Loss: 0.9371\n",
      "Epoch [7/50], Train Loss: 0.8092, Val Loss: 0.9204\n",
      "Epoch [8/50], Train Loss: 0.8130, Val Loss: 0.9166\n",
      "Epoch [9/50], Train Loss: 0.8874, Val Loss: 0.9627\n",
      "Epoch [10/50], Train Loss: 0.8781, Val Loss: 0.9472\n",
      "Epoch [11/50], Train Loss: 0.8135, Val Loss: 0.9648\n",
      "Epoch [12/50], Train Loss: 0.8330, Val Loss: 0.9747\n",
      "Epoch [13/50], Train Loss: 0.8038, Val Loss: 0.9551\n",
      "Epoch [14/50], Train Loss: 0.8383, Val Loss: 0.9745\n",
      "Epoch [15/50], Train Loss: 0.8474, Val Loss: 0.9810\n",
      "Epoch [16/50], Train Loss: 0.8326, Val Loss: 0.9726\n",
      "Epoch [17/50], Train Loss: 0.8169, Val Loss: 0.9716\n",
      "Epoch [18/50], Train Loss: 0.7831, Val Loss: 0.9809\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 11/20\n",
      "Parameters: {'lstm_hidden_size': 64, 'lstm_layers': 1, 'encoding_dim': 32, 'embedding_dim': 128, 'num_heads': 4, 'dropout': 0.3, 'batch_size': 32, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 1.0263, Val Loss: 0.8712\n",
      "Epoch [2/50], Train Loss: 0.8964, Val Loss: 1.0203\n",
      "Epoch [3/50], Train Loss: 0.8861, Val Loss: 1.0161\n",
      "Epoch [4/50], Train Loss: 0.9197, Val Loss: 0.9218\n",
      "Epoch [5/50], Train Loss: 0.8902, Val Loss: 1.0553\n",
      "Epoch [6/50], Train Loss: 0.8908, Val Loss: 0.9804\n",
      "Epoch [7/50], Train Loss: 0.7971, Val Loss: 0.9496\n",
      "Epoch [8/50], Train Loss: 0.8062, Val Loss: 0.8821\n",
      "Epoch [9/50], Train Loss: 0.8463, Val Loss: 0.8729\n",
      "Epoch [10/50], Train Loss: 0.8654, Val Loss: 0.8735\n",
      "Epoch [11/50], Train Loss: 0.8656, Val Loss: 0.9074\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 12/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 2, 'encoding_dim': 32, 'embedding_dim': 128, 'num_heads': 4, 'dropout': 0.1, 'batch_size': 32, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 1.0502, Val Loss: 0.9260\n",
      "Epoch [2/50], Train Loss: 0.8822, Val Loss: 0.8341\n",
      "Epoch [3/50], Train Loss: 0.9082, Val Loss: 0.8568\n",
      "Epoch [4/50], Train Loss: 0.8442, Val Loss: 0.8881\n",
      "Epoch [5/50], Train Loss: 0.8535, Val Loss: 0.8661\n",
      "Epoch [6/50], Train Loss: 0.8552, Val Loss: 0.8331\n",
      "Epoch [7/50], Train Loss: 0.8727, Val Loss: 0.9501\n",
      "Epoch [8/50], Train Loss: 0.8767, Val Loss: 0.8413\n",
      "Epoch [9/50], Train Loss: 0.8625, Val Loss: 0.9584\n",
      "Epoch [10/50], Train Loss: 0.8214, Val Loss: 0.8593\n",
      "Epoch [11/50], Train Loss: 0.7880, Val Loss: 0.8678\n",
      "Epoch [12/50], Train Loss: 0.8152, Val Loss: 0.8515\n",
      "Epoch [13/50], Train Loss: 0.8028, Val Loss: 0.8879\n",
      "Epoch [14/50], Train Loss: 0.7809, Val Loss: 0.8650\n",
      "Epoch [15/50], Train Loss: 0.7564, Val Loss: 0.9036\n",
      "Epoch [16/50], Train Loss: 0.7890, Val Loss: 0.8946\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 13/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 2, 'encoding_dim': 64, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.3, 'batch_size': 32, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 1.2107, Val Loss: 0.9087\n",
      "Epoch [2/50], Train Loss: 0.9087, Val Loss: 0.8659\n",
      "Epoch [3/50], Train Loss: 0.9436, Val Loss: 0.9312\n",
      "Epoch [4/50], Train Loss: 0.8660, Val Loss: 0.7788\n",
      "Epoch [5/50], Train Loss: 0.8779, Val Loss: 0.8463\n",
      "Epoch [6/50], Train Loss: 0.8992, Val Loss: 0.8052\n",
      "Epoch [7/50], Train Loss: 0.8425, Val Loss: 0.8045\n",
      "Epoch [8/50], Train Loss: 0.8762, Val Loss: 0.8509\n",
      "Epoch [9/50], Train Loss: 0.8900, Val Loss: 0.8062\n",
      "Epoch [10/50], Train Loss: 0.8403, Val Loss: 0.8368\n",
      "Epoch [11/50], Train Loss: 0.8585, Val Loss: 0.8017\n",
      "Epoch [12/50], Train Loss: 0.8387, Val Loss: 0.8022\n",
      "Epoch [13/50], Train Loss: 0.8235, Val Loss: 0.8160\n",
      "Epoch [14/50], Train Loss: 0.8256, Val Loss: 0.8171\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 14/20\n",
      "Parameters: {'lstm_hidden_size': 32, 'lstm_layers': 1, 'encoding_dim': 32, 'embedding_dim': 64, 'num_heads': 8, 'dropout': 0.3, 'batch_size': 16, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 0.9714, Val Loss: 0.9317\n",
      "Epoch [2/50], Train Loss: 0.8831, Val Loss: 0.9001\n",
      "Epoch [3/50], Train Loss: 0.8857, Val Loss: 0.9031\n",
      "Epoch [4/50], Train Loss: 0.8625, Val Loss: 0.8860\n",
      "Epoch [5/50], Train Loss: 0.8472, Val Loss: 0.8973\n",
      "Epoch [6/50], Train Loss: 0.8318, Val Loss: 0.9021\n",
      "Epoch [7/50], Train Loss: 0.8073, Val Loss: 0.8859\n",
      "Epoch [8/50], Train Loss: 0.8111, Val Loss: 0.8981\n",
      "Epoch [9/50], Train Loss: 0.8034, Val Loss: 0.9134\n",
      "Epoch [10/50], Train Loss: 0.8230, Val Loss: 0.9060\n",
      "Epoch [11/50], Train Loss: 0.7991, Val Loss: 0.9101\n",
      "Epoch [12/50], Train Loss: 0.7722, Val Loss: 0.9298\n",
      "Epoch [13/50], Train Loss: 0.7825, Val Loss: 0.9142\n",
      "Epoch [14/50], Train Loss: 0.7635, Val Loss: 0.9196\n",
      "Epoch [15/50], Train Loss: 0.7965, Val Loss: 0.9184\n",
      "Epoch [16/50], Train Loss: 0.7655, Val Loss: 0.9302\n",
      "Epoch [17/50], Train Loss: 0.7640, Val Loss: 0.9290\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 15/20\n",
      "Parameters: {'lstm_hidden_size': 64, 'lstm_layers': 2, 'encoding_dim': 64, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.3, 'batch_size': 32, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 1.1098, Val Loss: 1.3992\n",
      "Epoch [2/50], Train Loss: 0.9364, Val Loss: 1.0097\n",
      "Epoch [3/50], Train Loss: 0.9189, Val Loss: 1.0452\n",
      "Epoch [4/50], Train Loss: 0.8704, Val Loss: 1.0171\n",
      "Epoch [5/50], Train Loss: 0.8435, Val Loss: 1.0981\n",
      "Epoch [6/50], Train Loss: 0.9168, Val Loss: 1.0949\n",
      "Epoch [7/50], Train Loss: 0.8350, Val Loss: 1.0686\n",
      "Epoch [8/50], Train Loss: 0.8509, Val Loss: 1.1192\n",
      "Epoch [9/50], Train Loss: 0.8268, Val Loss: 1.1493\n",
      "Epoch [10/50], Train Loss: 0.8257, Val Loss: 1.1355\n",
      "Epoch [11/50], Train Loss: 0.8217, Val Loss: 1.1114\n",
      "Epoch [12/50], Train Loss: 0.8283, Val Loss: 1.1073\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 16/20\n",
      "Parameters: {'lstm_hidden_size': 32, 'lstm_layers': 2, 'encoding_dim': 64, 'embedding_dim': 64, 'num_heads': 8, 'dropout': 0.1, 'batch_size': 32, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 1.2095, Val Loss: 0.7815\n",
      "Epoch [2/50], Train Loss: 0.9106, Val Loss: 0.7706\n",
      "Epoch [3/50], Train Loss: 0.8613, Val Loss: 0.8278\n",
      "Epoch [4/50], Train Loss: 0.8709, Val Loss: 0.8197\n",
      "Epoch [5/50], Train Loss: 0.8459, Val Loss: 0.7600\n",
      "Epoch [6/50], Train Loss: 0.8443, Val Loss: 0.7843\n",
      "Epoch [7/50], Train Loss: 0.8362, Val Loss: 0.8104\n",
      "Epoch [8/50], Train Loss: 0.8438, Val Loss: 0.8392\n",
      "Epoch [9/50], Train Loss: 0.8550, Val Loss: 0.7701\n",
      "Epoch [10/50], Train Loss: 0.8748, Val Loss: 0.8518\n",
      "Epoch [11/50], Train Loss: 0.8180, Val Loss: 0.7816\n",
      "Epoch [12/50], Train Loss: 0.8587, Val Loss: 0.8284\n",
      "Epoch [13/50], Train Loss: 0.8342, Val Loss: 0.7879\n",
      "Epoch [14/50], Train Loss: 0.8476, Val Loss: 0.7855\n",
      "Epoch [15/50], Train Loss: 0.8021, Val Loss: 0.7732\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 17/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 1, 'encoding_dim': 64, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.2, 'batch_size': 16, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 1.1597, Val Loss: 0.7256\n",
      "Epoch [2/50], Train Loss: 0.9585, Val Loss: 0.8269\n",
      "Epoch [3/50], Train Loss: 0.8780, Val Loss: 0.6583\n",
      "Epoch [4/50], Train Loss: 0.8915, Val Loss: 0.6844\n",
      "Epoch [5/50], Train Loss: 0.8720, Val Loss: 0.6654\n",
      "Epoch [6/50], Train Loss: 0.9415, Val Loss: 0.8097\n",
      "Epoch [7/50], Train Loss: 0.8696, Val Loss: 0.7117\n",
      "Epoch [8/50], Train Loss: 0.8429, Val Loss: 0.7267\n",
      "Epoch [9/50], Train Loss: 0.8608, Val Loss: 0.7531\n",
      "Epoch [10/50], Train Loss: 0.8543, Val Loss: 0.7434\n",
      "Epoch [11/50], Train Loss: 0.8547, Val Loss: 0.6934\n",
      "Epoch [12/50], Train Loss: 0.8391, Val Loss: 0.7278\n",
      "Epoch [13/50], Train Loss: 0.8199, Val Loss: 0.7125\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 18/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 2, 'encoding_dim': 64, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.2, 'batch_size': 32, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 1.3504, Val Loss: 1.3889\n",
      "Epoch [2/50], Train Loss: 0.9108, Val Loss: 0.7228\n",
      "Epoch [3/50], Train Loss: 0.8663, Val Loss: 0.6248\n",
      "Epoch [4/50], Train Loss: 0.8632, Val Loss: 0.6021\n",
      "Epoch [5/50], Train Loss: 0.8932, Val Loss: 0.6588\n",
      "Epoch [6/50], Train Loss: 0.8663, Val Loss: 0.5995\n",
      "Epoch [7/50], Train Loss: 0.8644, Val Loss: 0.6319\n",
      "Epoch [8/50], Train Loss: 0.8681, Val Loss: 0.6697\n",
      "Epoch [9/50], Train Loss: 0.8452, Val Loss: 0.6417\n",
      "Epoch [10/50], Train Loss: 0.8846, Val Loss: 0.7521\n",
      "Epoch [11/50], Train Loss: 0.8592, Val Loss: 0.6039\n",
      "Epoch [12/50], Train Loss: 0.8771, Val Loss: 0.6084\n",
      "Epoch [13/50], Train Loss: 0.8630, Val Loss: 0.5828\n",
      "Epoch [14/50], Train Loss: 0.8279, Val Loss: 0.5661\n",
      "Epoch [15/50], Train Loss: 0.8146, Val Loss: 0.6280\n",
      "Epoch [16/50], Train Loss: 0.8217, Val Loss: 0.5657\n",
      "Epoch [17/50], Train Loss: 0.8609, Val Loss: 0.6521\n",
      "Epoch [18/50], Train Loss: 0.7964, Val Loss: 0.7676\n",
      "Epoch [19/50], Train Loss: 0.8763, Val Loss: 0.8597\n",
      "Epoch [20/50], Train Loss: 0.8732, Val Loss: 0.8058\n",
      "Epoch [21/50], Train Loss: 0.8284, Val Loss: 0.5982\n",
      "Epoch [22/50], Train Loss: 0.8340, Val Loss: 0.6481\n",
      "Epoch [23/50], Train Loss: 0.8190, Val Loss: 0.5778\n",
      "Epoch [24/50], Train Loss: 0.8454, Val Loss: 0.6530\n",
      "Epoch [25/50], Train Loss: 0.8014, Val Loss: 0.5942\n",
      "Epoch [26/50], Train Loss: 0.7963, Val Loss: 0.5733\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 19/20\n",
      "Parameters: {'lstm_hidden_size': 32, 'lstm_layers': 1, 'encoding_dim': 32, 'embedding_dim': 64, 'num_heads': 8, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.001}\n",
      "Epoch [1/50], Train Loss: 1.1709, Val Loss: 1.1381\n",
      "Epoch [2/50], Train Loss: 0.9536, Val Loss: 0.9386\n",
      "Epoch [3/50], Train Loss: 0.8863, Val Loss: 0.8589\n",
      "Epoch [4/50], Train Loss: 0.8669, Val Loss: 0.8455\n",
      "Epoch [5/50], Train Loss: 0.8626, Val Loss: 0.8299\n",
      "Epoch [6/50], Train Loss: 0.8490, Val Loss: 0.8624\n",
      "Epoch [7/50], Train Loss: 0.8355, Val Loss: 0.8364\n",
      "Epoch [8/50], Train Loss: 0.8561, Val Loss: 0.9079\n",
      "Epoch [9/50], Train Loss: 0.8248, Val Loss: 0.8681\n",
      "Epoch [10/50], Train Loss: 0.8253, Val Loss: 0.8839\n",
      "Epoch [11/50], Train Loss: 0.8047, Val Loss: 0.9355\n",
      "Epoch [12/50], Train Loss: 0.8171, Val Loss: 0.9413\n",
      "Epoch [13/50], Train Loss: 0.7998, Val Loss: 0.8818\n",
      "Epoch [14/50], Train Loss: 0.7784, Val Loss: 0.8761\n",
      "Epoch [15/50], Train Loss: 0.7834, Val Loss: 0.9556\n",
      "Early stopping triggered\n",
      "\n",
      "Trial 20/20\n",
      "Parameters: {'lstm_hidden_size': 128, 'lstm_layers': 2, 'encoding_dim': 32, 'embedding_dim': 128, 'num_heads': 8, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.0001}\n",
      "Epoch [1/50], Train Loss: 0.9463, Val Loss: 0.9142\n",
      "Epoch [2/50], Train Loss: 0.9574, Val Loss: 0.8020\n",
      "Epoch [3/50], Train Loss: 0.8674, Val Loss: 0.7862\n",
      "Epoch [4/50], Train Loss: 0.8631, Val Loss: 0.7437\n",
      "Epoch [5/50], Train Loss: 0.8930, Val Loss: 0.7539\n",
      "Epoch [6/50], Train Loss: 0.8437, Val Loss: 0.7671\n",
      "Epoch [7/50], Train Loss: 0.8431, Val Loss: 0.8148\n",
      "Epoch [8/50], Train Loss: 0.8329, Val Loss: 0.7641\n",
      "Epoch [9/50], Train Loss: 0.8188, Val Loss: 0.7679\n",
      "Epoch [10/50], Train Loss: 0.8127, Val Loss: 0.7524\n",
      "Epoch [11/50], Train Loss: 0.8002, Val Loss: 0.7703\n",
      "Epoch [12/50], Train Loss: 0.7941, Val Loss: 0.7881\n",
      "Epoch [13/50], Train Loss: 0.7750, Val Loss: 0.7625\n",
      "Epoch [14/50], Train Loss: 0.7902, Val Loss: 0.8165\n",
      "Early stopping triggered\n",
      "\n",
      "Best parameters found: {'lstm_hidden_size': 128, 'lstm_layers': 2, 'encoding_dim': 64, 'embedding_dim': 64, 'num_heads': 4, 'dropout': 0.2, 'batch_size': 32, 'learning_rate': 0.001}\n",
      "Best validation loss: 0.5656781177967787\n"
     ]
    }
   ],
   "source": [
    "best_params, best_model_state, results = random_search(train_dataset, device, n_trials=20)\n",
    "best_random_val_loss = min(result['val_loss'] for result in results)\n",
    "print(\"\\nBest parameters found:\", best_params)\n",
    "print(\"Best validation loss:\", best_random_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bR7qsgjq-RYM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search found better model\n"
     ]
    }
   ],
   "source": [
    "# Check if the best model from random search is better than the base model\n",
    "if best_random_val_loss < base_val_loss:\n",
    "    print(\"Random search found better model\")\n",
    "    \n",
    "    # Initialize CGM encoder with best parameters\n",
    "    cgm_encoder = CGMEncoder(\n",
    "        hidden_size=best_params['lstm_hidden_size'],\n",
    "        num_layers=best_params['lstm_layers'],\n",
    "        output_size=best_params['encoding_dim']\n",
    "    ).to(device)\n",
    "\n",
    "    # Initialize image encoder with best parameters\n",
    "    image_encoder = ImageEncoder(output_size=best_params['encoding_dim']).to(device)\n",
    "    \n",
    "    # Initialize demographic encoder with best parameters\n",
    "    demo_encoder = DemoEncoder(output_size=best_params['encoding_dim']).to(device)\n",
    "\n",
    "    # Initialize multimodal fusion model with best parameters\n",
    "    multimodal_model = MultimodalAttentionFusion(\n",
    "        encoding_dim=best_params['encoding_dim'],\n",
    "        embedding_dim=best_params['embedding_dim'],\n",
    "        num_heads=best_params['num_heads'],\n",
    "        dropout=best_params['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the best model states for each component\n",
    "    cgm_encoder.load_state_dict(best_model_state['cgm_encoder'])\n",
    "    image_encoder.load_state_dict(best_model_state['image_encoder'])\n",
    "    demo_encoder.load_state_dict(best_model_state['demo_encoder'])\n",
    "    multimodal_model.load_state_dict(best_model_state['multimodal_model'])\n",
    "else:\n",
    "    print(\"Base model performed better\")\n",
    "    # Load the base model states for each component\n",
    "    cgm_encoder.load_state_dict(base_model_state['cgm_encoder'])\n",
    "    image_encoder.load_state_dict(base_model_state['image_encoder'])\n",
    "    demo_encoder.load_state_dict(base_model_state['demo_encoder'])\n",
    "    multimodal_model.load_state_dict(base_model_state['multimodal_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_KGky9pZ-UnZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "final_Lunch Predictions:\n",
      "   row_id       label\n",
      "0       0  513.246399\n",
      "1       1  497.122498\n",
      "2       2  502.132660\n",
      "3       3  578.931885\n",
      "4       4  545.885254\n"
     ]
    }
   ],
   "source": [
    "# Set all model components to evaluation mode\n",
    "cgm_encoder.eval()\n",
    "image_encoder.eval()\n",
    "demo_encoder.eval()\n",
    "multimodal_model.eval()\n",
    "\n",
    "# Initialize list to store lunch predictions\n",
    "lunch_predictions = []\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    # Iterate through batches in the test loader\n",
    "    for batch in test_loader:\n",
    "        # Move batch data to the appropriate device (CPU or GPU)\n",
    "        cgm_data = batch['cgm'].to(device)\n",
    "        demo_data = batch['demo'].to(device)\n",
    "        breakfast_img = batch['breakfast_img'].to(device)\n",
    "        lunch_img = batch['lunch_img'].to(device)\n",
    "\n",
    "        # Encode each modality of data\n",
    "        cgm_encoded = cgm_encoder(cgm_data)\n",
    "        demo_encoded = demo_encoder(demo_data)\n",
    "        breakfast_encoded = image_encoder(breakfast_img)\n",
    "        lunch_encoded = image_encoder(lunch_img)\n",
    "\n",
    "        # Generate predictions using the multimodal model\n",
    "        predictions = multimodal_model(\n",
    "            cgm_encoded,\n",
    "            demo_encoded,\n",
    "            breakfast_encoded,\n",
    "            lunch_encoded\n",
    "        )\n",
    "\n",
    "        # Append predictions to the list, moving them back to CPU\n",
    "        lunch_predictions.append(predictions.cpu())\n",
    "\n",
    "# Concatenate all predictions into a single tensor and convert to numpy array\n",
    "final_lunch_predictions = torch.cat(lunch_predictions, dim=0).numpy()\n",
    "\n",
    "# Inverse transform the predictions to get original scale lunch calorie values\n",
    "final_lunch_predictions = preprocessor.label_scaler.inverse_transform(final_lunch_predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create a DataFrame with row IDs and predicted lunch calories\n",
    "lunch_df = pd.DataFrame({\n",
    "    'row_id': range(len(final_lunch_predictions)),\n",
    "    'label': final_lunch_predictions\n",
    "})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "lunch_df.to_csv('final_lunch_predictions.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the predictions\n",
    "print(\"\\nfinal_Lunch Predictions:\")\n",
    "print(lunch_df.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
